{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import os \n",
    "import config\n",
    "import prompts\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "import  nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "\n",
    "source_pdf = pd.read_pickle(os.path.join(os.getcwd(), \"..\", \"data\", \"eval\", \"src_example.pkl\"))\n",
    "data = pd.read_pickle(os.path.join(os.getcwd(), \"..\", \"data\", \"eval\", \"example.pkl\"))\n",
    "data['sentences'] = source_pdf[data['indices']].reset_index(drop = True)\n",
    "\n",
    "def join_sentences(row):\n",
    "    index = row['indices']\n",
    "    context = config.CONTEXT_SENTENCES\n",
    "    sentences = source_pdf[index - context if index >= context else 0 : index]\n",
    "    context =  ''.join([s + '\\n' if len(s) != 0 and s[-1] not \\\n",
    "                     in string.punctuation else s + ' ' for s in sentences])\n",
    "    return context\n",
    "\n",
    "def create_prompt(row):\n",
    "    error = row['errors']\n",
    "    context = row['contexts']\n",
    "    sentence = row['sentences']\n",
    "    prompt_template = prompts.ERRORS[error].format(context = context,\n",
    "                                                  completion = sentence)\n",
    "    return prompt_template\n",
    "\n",
    "data['contexts'] = data.apply(join_sentences, axis=1)\n",
    "data['prompts'] = data.apply(create_prompt, axis = 1)\n",
    "\n",
    "data = data.to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.llms import HuggingFaceHub\n",
    "\n",
    "# hf_model_name = 'HuggingFaceH4/zephyr-7b-alpha'\n",
    "# # openai = OpenAI(model_name='gpt-3.5-turbo-instruct', max_tokens=-1)\n",
    "# hf = HuggingFaceHub(repo_id=hf_model_name, model_kwargs= {'max_new_tokens': 250,\n",
    "#                                                           })\n",
    "# outputs = []\n",
    "# for i, prompt in enumerate(data['prompts']):\n",
    "#     output = hf.invoke(data['prompts'][i])\n",
    "#     outputs.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the outputs into multiple completion choices\n",
    "def parse_text(text):\n",
    "    lines = text.split('\\n')\n",
    "    hal_choices = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(lines):\n",
    "        line = lines[i].strip()\n",
    "\n",
    "        # Check if the line starts with a number\n",
    "        if line and line[0].isdigit():\n",
    "            # Check if the line contains \"Change:\"\n",
    "            if \"Change:\" in line:\n",
    "                i += 1\n",
    "\n",
    "                # Check if the next line contains \"Contradiction\"\n",
    "                if i < len(lines) and \"Contradiction:\" in lines[i]:\n",
    "                    contradiction = lines[i].split(\"Contradiction:\", 1)[1].strip()\n",
    "                    i += 1\n",
    "                    hal_choices.append(contradiction)\n",
    "            else:\n",
    "                # Line starts with a number but doesn't contain \"Change:\", move to the next line\n",
    "                i += 1\n",
    "        else:\n",
    "            # Line doesn't start with a number, move to the next line\n",
    "            i += 1\n",
    "\n",
    "    return hal_choices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['choices'] = [parse_text(text) for text in outputs]\n",
    "import json\n",
    "with open('data.json', 'r') as json_file:\n",
    "    data['choices'] =  json.load(json_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import AsyncInferenceClient\n",
    "from transformers import AutoTokenizer\n",
    "import asyncio\n",
    "import numpy as np\n",
    "\n",
    "API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "MODEL = 'HuggingFaceH4/zephyr-7b-alpha'\n",
    "\n",
    "MIN_PROP = 0.7\n",
    "data['choices'] = [[choice.strip() for choice in choices if len(choice) >= MIN_PROP * len(s)] for s, \\\n",
    "                   choices in zip(data['sentences'], data['choices'])]\n",
    "\n",
    "# Check if two sentences contradict\n",
    "async def contradictory_score(i, sentence, choices, model='microsoft/deberta-v2-xlarge-mnli'):\n",
    "    if len(choices) == 0:\n",
    "        return i, []\n",
    "    prompts = ['[CLS]' + sentence + '[SEP]' + choice + '[SEP]' for choice in choices]\n",
    "    client = AsyncInferenceClient(model=model, token= API_TOKEN)\n",
    "    results = [await client.text_classification(prompt) for prompt in prompts]\n",
    "    contradiction_scores = []\n",
    "    # Iterate through the list and extract scores for 'CONTRADICTION'\n",
    "    for result in results:\n",
    "        for item in result:\n",
    "            if item['label'] == 'CONTRADICTION':\n",
    "                contradiction_scores.append(item['score'])\n",
    "    \n",
    "    return i, contradiction_scores\n",
    "\n",
    "async def get_nml_score(template, context, context_len, choice, model=MODEL):\n",
    "    prompt = template.format(c=context, s=choice)\n",
    "    client = AsyncInferenceClient(model=model, token=API_TOKEN)\n",
    "    response = await client.text_generation(prompt, max_new_tokens=1,\n",
    "                                                details=True, decoder_input_details=True)\n",
    "    logit_scores = [x.logprob for x in response.details.prefill[context_len:]]\n",
    "    try:\n",
    "        nml_score = sum(logit_scores) / len(logit_scores)\n",
    "    except Exception as e:\n",
    "        print(context)\n",
    "        print()\n",
    "        print('Here are the choices')\n",
    "        print(choice)\n",
    "        print(len(choice))\n",
    "        return e\n",
    "    return nml_score\n",
    "\n",
    "async def fluency_score_difference(i, sentence, context, context_len, choices):    \n",
    "    if len(choices) == 0:\n",
    "        return i, []\n",
    "    print(i)\n",
    "    template = '{c} {s}'\n",
    "    # Normalized logit for original prompt\n",
    "    org_prompt_n_logit = await get_nml_score(template, context, context_len, sentence)\n",
    "    # Normalized logits for choices\n",
    "    prompt_choices_logits = [await get_nml_score(template, context, context_len, choice) for choice in choices]\n",
    "    # Get the difference\n",
    "    return i, (np.array(prompt_choices_logits) - org_prompt_n_logit).tolist()\n",
    "\n",
    "start = 0\n",
    "limit = 10\n",
    "end = start + limit\n",
    "\n",
    "contradictory_scores = []\n",
    "fluency_score_differences = []\n",
    "\n",
    "async def process_data(start, end):\n",
    "    global contradictory_scores\n",
    "    global fluency_score_differences\n",
    "    # Get the contradictory score:\n",
    "    results_0 = await asyncio.gather(*[contradictory_score(i + start, s, c) for i, (s, c) in enumerate(zip(data['sentences'][start:end], data['choices'][start:end]))])\n",
    "    # Get the fluency difference between choice completion and original completion.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "    context_lens = [len(x) for x in tokenizer(data['contexts'][start:end])['input_ids']]\n",
    "    results_1 = await asyncio.gather(*[fluency_score_difference(i + start, s, ct, ctl, c) for i, (s, ct, ctl, c) in enumerate(zip(data['sentences'][start:end], data['contexts'][start:end], context_lens, data['choices'][start:end]))])\n",
    "    contradictory_scores += results_0\n",
    "    fluency_score_differences += results_1\n",
    "\n",
    "async def main():\n",
    "    global start\n",
    "    global end\n",
    "    global data\n",
    "    while start < len(data['errors']):\n",
    "        await process_data(start, end)\n",
    "        start += 10\n",
    "        end = min(start + limit, len(data['errors']))\n",
    "\n",
    "    data['contradiction_scores'] = [x[1] for x in sorted(contradictory_scores, key=lambda x: x[0])]\n",
    "    data['fluency_score_difference'] = [x[1] for x in sorted(fluency_score_differences, key=lambda x: x[0])]\n",
    "\n",
    "asyncio.run(main())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter usable data completion\n",
    "\n",
    "## if error is not coreference then contradiction score needs\n",
    "## to be more than phi_a_n else phi_a_c. Fluency difference\n",
    "## need to be more than phi_b (should be negative number)\n",
    "PHI_A_N = 0.6\n",
    "PHI_A_C = 0.3\n",
    "PHI_B = -0.25\n",
    "\n",
    "def selected_choices(error, c_scores, f_score_diffs, choices):\n",
    "    filtered_choices = []\n",
    "    for i,choice in enumerate(choices):\n",
    "        if f_score_diffs[i] < PHI_B:\n",
    "            continue\n",
    "        if error == 'COREFERENCE':\n",
    "            if c_scores[i] >= PHI_A_C:\n",
    "                filtered_choices.append(choice)\n",
    "        else:\n",
    "            if c_scores[i] >= PHI_A_N:\n",
    "                filtered_choices.append(choice)\n",
    "    return filtered_choices\n",
    "\n",
    "filter_choices = [selected_choices(e, c_s, f_s_d, c) for e,c_s, f_s_d, c \\\n",
    "                  in zip(data['errors'], data['contradiction_scores'], data['fluency_score_difference'],\n",
    "                         data['choices'])]\n",
    "chosen_data_indices = [i for i,c in enumerate(filter_choices) if len(c) != 0 ]\n",
    "\n",
    "chosen_data = {\n",
    "    'errors': [data['errors'][i] for i in chosen_data_indices],\n",
    "    'contexts': [data['contexts'][i] for i in chosen_data_indices],\n",
    "    'sentences': [data['sentences'][i] for i in chosen_data_indices],\n",
    "    'choices': [filter_choices[i] for i in chosen_data_indices],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TextGenerationResponse(generated_text=\"ðŸ‘‹\\n\\nI'm a newbie to the world of programming and I'\", details=Details(finish_reason=<FinishReason.Length: 'length'>, generated_tokens=20, seed=None, prefill=[], tokens=[Token(id=243, text='', logprob=-1.1025391, special=False), Token(id=162, text='', logprob=-0.0001193285, special=False), Token(id=148, text='', logprob=-0.83496094, special=False), Token(id=142, text='', logprob=-0.0074272156, special=False), Token(id=13, text='', logprob=-0.9902344, special=False), Token(id=13, text='\\n\\n', logprob=-0.21704102, special=False), Token(id=28737, text='I', logprob=-1.1689453, special=False), Token(id=28742, text=\"'\", logprob=-1.3085938, special=False), Token(id=28719, text='m', logprob=-0.21057129, special=False), Token(id=264, text=' a', logprob=-2.0605469, special=False), Token(id=633, text=' new', logprob=-2.5039062, special=False), Token(id=12868, text='bie', logprob=-1.3720703, special=False), Token(id=298, text=' to', logprob=-1.0986328, special=False), Token(id=272, text=' the', logprob=-1.6582031, special=False), Token(id=1526, text=' world', logprob=-2.0820312, special=False), Token(id=302, text=' of', logprob=-0.008110046, special=False), Token(id=16292, text=' programming', logprob=-2.8457031, special=False), Token(id=304, text=' and', logprob=-0.7832031, special=False), Token(id=315, text=' I', logprob=-1.0390625, special=False), Token(id=28742, text=\"'\", logprob=-0.47314453, special=False)], best_of_sequences=None))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from huggingface_hub import InferenceClient\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "API_TOKEN = os.getenv('HUGGINGFACEHUB_API_TOKEN')\n",
    "client = InferenceClient(model='mistralai/Mistral-7B-Instruct-v0.1')\n",
    "client.text_generation('Hello ', details=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
